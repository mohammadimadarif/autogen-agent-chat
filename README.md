# autogen-agent-chat
Overview

This project implements an AI agent chat system using AutoGen and Ollama. The system enables multi-agent conversations and uses a local LLM model (tinyllama) to process and respond to queries.

ğŸš€ Features

Uses AutoGen for conversational AI agents.

Runs a local LLM model (tinyllama) via Ollama.

Supports multi-agent interactions.

Includes a user proxy agent to interact with the assistant.

Allows for code execution and tool integration.

ğŸ› ï¸ Prerequisites

1ï¸âƒ£ Install Python and Virtual Environment

Ensure you have Python 3.8+ installed. Then, create and activate a virtual environment:

2ï¸âƒ£ Install Required Dependencies

3ï¸âƒ£ Install & Run Ollama

Download and install Ollama from Ollama's official website. After installation, start the Ollama service:

ğŸ—ï¸ Project Setup

Clone this repository:

Install dependencies:

Run the AI agent chat system:

ğŸ”¬ Testing

You can start with a simple query:

ğŸ”§ Troubleshooting

If you get an Ollama model not found error, run:

If the server isnâ€™t running, start it manually:

ğŸ“ License

This project is licensed under the MIT License. Feel free to modify and use it as needed.

ğŸ¤ Contributing

Pull requests are welcome! If you have suggestions or improvements, please open an issue.
